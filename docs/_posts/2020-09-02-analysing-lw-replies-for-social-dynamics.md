---
title: Analysing some LW replies for social dynamics
date: 2020-09-02
parent: Posts
layout: post
---

1. TOC
{:toc}

## Donald Hobson

[Donald Hobson's reply](https://www.lesswrong.com/posts/Ze6PqJK2jnwnhcpnb/chains-bottlenecks-and-optimization?commentId=3oJLTCiDswNjEFrzk)

I didn't spot any explicit social stuff on the first pass looking for particular works/phrases that are red flags.

### Second pass and comparing to list of questions about social dynamics:

DH is putting in some effort, though he's making general comments rather than responding to curi particularly. He's not asking Qs to try and understand more. He's expecting a reaction, like "tell me why I'm wrong" rather than "I think there's a problem with your idea".

There's no direct response to what curi wrote, but there could have been. e.g. DH could have quoted "How strong is a chain? How hard can you pull on it before it breaks? It’s as strong as its weakest link." or something, that would have been relevant.

DH also contradicts curi without arguing against curi's case, just stating an alternative. e.g curi said:

> If you measure the strength of every link in the chain, and try to combine them into an overall strength score for the chain, you will get a bad answer.

And DH replied:

> On the other hand, Cojunctive arguments are weaker than the weakest link. If I have an argument that has 100 steps, and relies on all the steps holding for the argument to hold, then even if each step is 99% certain, the whole argument is only 37% certain.

It sounds like he's contradicting curi but he doesn't say so or say what the contradiction is. This means curi needs to reply questioning DH, instead of DH questioning curi - it's a tactic that shifts DH into a higher social position b/c now curi needs to do the work to figure out what DH meant or point out why DH is wrong (making curi expend more effort).

DH is also not explaining himself fully, giving references, or explaining why curi is wrong. So this is DH giving hints and expecting curi to figure things out.

DH also doesn't look needy, rather he looks calm and somewhat casual. He's a bit loose with some technical references (indicating LoLE and signalling expertise).

DH is re-framing the issue to be "refute DH" rather than "refute curi" which is again a social status play.

### Donald Hobson's full reply

> I think that ideas can have a bottleneck effect, but that isn't the only effect. Some ideas have disjunctive justifications. You might have genetic evidence, and embryological evidence, and studies of bacteria evolving in test tubes, and a fossil record, all pointing towards the theory of evolution. In this case, the chains are all in parallel, so you can add their strength. Even if all the fossil record is completely discounted, you would still have good genetic evidence, and visa versa. If, conditional on the strongest line of evidence being totally discounted, you would still expect the other lines might hold, then the combination is stronger than the strongest.
>
> If I have 10 arguments that X is true, and each argument is 50/50 (I assign equal probability to the argument being totally valid, and totally useless) And if these arguments are totally independant in their validity, then P(not X) < 1/1000. Note that total independance means that even when told with certainty that arguments 0 ... 8 are nonsense, you still assign 50% prob to argument 9 being valid. This is a strong condition.
>
> Disjunctive arguments are stronger than the strongest link.
>
> On the other hand, Cojunctive arguments are weaker than the weakest link. If I have an argument that has 100 steps, and relies on all the steps holding for the argument to hold, then even if each step is 99% certain, the whole argument is only 37% certain.
>
> For those of you looking for a direct numeric analogue to resistors, there isn't one. They have similar structure, resistors with x+y and 1/x, probabilities with x\*y and 1-x, but there isn't a (nice) function from one to another. There isn't f st f(x+y)=f(x)\*f(y) and f(1/x)=1-f(x)

## Dagon

[Dagon's reply](https://www.lesswrong.com/posts/Ze6PqJK2jnwnhcpnb/chains-bottlenecks-and-optimization?commentId=J3ht5uAtAmGzd3PCE)

curi and I discussed this a bit in t34. I'll mention everything I see tho, but I'm not going to put in as much effort over all as fresh examples.

### first pass - low level

Dagon starts with some statements about himself / self-aggrandisement. There's a weird apology. He then contradicts the first paragraph somewhat.

Dagon then has a few short statements which challenge curi's post but only semi-directly. He doesn't quote anything, and there's some reframing going on. He states some things like they're fact which signals expertise and LoLE.

### second pass

Dagon's first paragraph is setting a social scene (that he's high status and has expertise). he knows lots about goldratt because he's given away "the goal" and wants to keep recommending it or whatever.

Dagon is pointing things out in a teacher-esq sort of way, e.g.

> It's important to recognize the limits of the chain metaphor - there is variance/uncertainty in the strength of a link (or capacity of a production step), and variance/uncertainty in alternate support for ideas (or alternate production paths).

He's communicating that he's an expert and that the's pointing out something curi has missed. This is reframing, and potentially outright dishonest if he read the whole post, and negligent dishonest if he didn't (b/c he's acting like it).

He's also communicating that he *knows more than Goldratt*, that he's pointing something out that Goldratt got wrong. Which begs the questions: "why does he recommend it? are there not better things to recommend?"

(Note: ET pointed out Dagon might think Goldratt agrees with him about the limits of the chain metaphor)

He's expecting curi to respond with more detail or something and on the whole it's a low-effort response (both LoLE and actually LE). The conversation is now about his idea.

(This reframing to talk about your own ideas feels like it's going to be pretty common).

### Dagon's full reply

> I think I've given away over 20 copies of _The Goal_ by Goldratt, and recommended it to coworkers hundreds of times. Thanks for the chance to recommend it again - it's much more approachable than _Theory of Constraints_, and is more entertaining, while still conveying enough about his worldview to let you decide if you want the further precision and examples in his other books.
>
> It's important to recognize the limits of the chain metaphor - there is variance/uncertainty in the strength of a link (or capacity of a production step), and variance/uncertainty in alternate support for ideas (or alternate production paths). Most real-world situations are more of a mesh or a circuit than a linear chain, and the analysis of bottlenecks and risks is a fun multidimensional calculation of forces applies and propagated through multiple links.

## abramdemski

[abramdemski's reply](https://www.lesswrong.com/posts/Ze6PqJK2jnwnhcpnb/chains-bottlenecks-and-optimization?commentId=9yutShzHmkLogPetC)

### first pass

immediately: this is a longer reply (the longest I think) and has quotes.

He quotes curi and then acts like he's going to point out a problem (will he?):

>>There are special cases. Maybe the links are all equally strong to high precision. But that’s unusual. Variance (statistical fluctuations) is usual. Perhaps there is a bell curve of link strengths. Having two links approximately tied for weakest is more realistic, though still uncommon.
>
>There is another case which your argument neglects, which can make weakest-link reasoning highly inaccurate, and which is less of a special case than a tie in link-strength.

(note: that's the end of abramdemski's paragraph - it's just that one sentence)

he points out the conjunctive/disjunctive thing. There's a bit of condescension when he says stuff like:

> (Of course there are even more exotic logical connectives, such as implication or XOR, which are also used in everyday reasoning. But for now it will do to consider only conjunction and disjunction.)

it's condescending because it's basic (like wow, implication and XOR, such advanced concepts). this is reframing to a degree, treating curi like he doesn't know what he's talking about (relative status elevation).

> Following the venerated [method of multiple working hypotheses](https://www.lesswrong.com/posts/2mvSvjbsA7kqfNxZW/bayes-law-is-about-multiple-hypothesis-testing), then, we are well-advised to come up with as many hypotheses as we can to explain the data. [...]

he's appealing to the status of some idea and implying that curi is dumb for contradicting it, while associating himself. he's reframing because he's not asking if there's like a contradiction or something, he's just stating it as fact. it's now the responsibility of curi to refute him and the method of multiple working hypotheses (or point out why there's not a contradiction).

he also says we're "well-advised" which is adding extra judgement (implying his judgement is worth something extra) to the idea. this is strictly unnecessary, he could just say "we are advised".

He's also talking in "we"s, which reframes the conversation to 'curi vs the mob' or similar.

abramdemski continues to quote curi, but then offers a false concession/agreement with:

> I can agree that a weighted average is not what is called for in a conjunctive case. It is closer to correct in a disjunctive case.

That's not really substantial; he's not saying curi's right, just that curi's not super wrong, that *some* part is good. he then sort of contradicts himself with the second sentence, but they're both similar in the way they treat curi/curi's idea. both sentences are more signalling of expertise and are designed to make abramdemski look better (like more magnanimous or generous or something), while making curi look like he's missed something and sorta 'on the right track'.

abramdemski says:

> But let us focus on your claim that we should ignore non-bottlenecks. You already praised ideas for having excess capacity, IE, being more accurate than needed for a given application. But now you are criticizing a practice of weighing evidence too accurately.

he puts the word "praised" in curi's mouth. he's also misrepresented curi with "more accurate than needed", which directly contradicts curi's statement: "Excess capacity means the idea is more than adequate to accomplish its purpose." I doubt abramdemski realised he did this - it looks like a mistake ppl who hold his views would make b/c they're interpreting what curi said through their own lense (which ignores curi's statements: "I expect large inferential distance. I don’t expect my intended meaning to be transparent to readers here")

this is designed to reframe the conversation to be about what abramdemski is saying.

like a lot of comments abramdemski brings up probability even tho curi didn't mention it.

abramdemski then quotes curi saying "Excess capacity means..." which indicates he is either ignoring the contradiction mentioned above or didn't understand it. he pretends (either intentionally or b/c he's fooling himself) that he did, though.

he then finishes with some questions that are either using terms abramdemski introduced, or already answered by curi in the post. he's expecting curi to put more effort in to understand why curi is wrong and implicitly demanding a reframing of the topic to be about what he thinks it is, not what curi posted. He's also implicitly demanding curi reply to those Qs particularly, even tho none of the 3 are good questions.

### abramdemski's full reply

>> There are special cases. Maybe the links are all equally strong to high precision. But that’s unusual. Variance (statistical fluctuations) is usual. Perhaps there is a bell curve of link strengths. Having two links approximately tied for weakest is more realistic, though still uncommon.
>
>There is another case which your argument neglects, which can make weakest-link reasoning highly inaccurate, and which is less of a special case than a tie in link-strength.
>
> The way you are reasoning about systems of interconnected ideas is conjunctive: every individual thing needs to be true. But some things are disjunctive: some one thing needs to be true. (Of course there are even more exotic logical connectives, such as implication or XOR, which are also used in everyday reasoning. But for now it will do to consider only conjunction and disjunction.)
>
> A conjunction of a number of statements is -- at most -- as strong as its weakest element, as you suggest. However, a disjunction of a number of statements is -- at worst -- as strong as its strongest element.
>
> Following the venerated method of multiple working hypotheses, then, we are well-advised to come up with as many hypotheses as we can to explain the data. Doing this has many advantages, but one worth mentioning is that it increases the probability that one of our ideas is right.
>
> Since we don't want to just believe everything, we then have to somehow weigh the different hypotheses against each other. Our method of weighing hypotheses should ideally have the property that, if the correct hypothesis is in our collection, we would eventually converge to it. (Having established that, we might ask to converge as quickly as possible; and we could also name many other desirable properties of such a weighting scheme.)
>
> Concerning such weighting schemes, you comment:
>
>> Don’t update your beliefs using evidence, increasing your confidence in some ideas, when the evidence deals with non-bottlenecks. In other words, don’t add more plausibility to an idea when you improve a sub-component that already had excess capacity. Don’t evaluate the quality of all the components of an idea and combine them into a weighted average which comes out higher when there’s more excess capacity for non-bottlenecks.
>
> I can agree that a weighted average is not what is called for in a conjunctive case. It is closer to correct in a disjunctive case. But let us focus on your claim that we should ignore non-bottlenecks. You already praised ideas for having excess capacity, IE, being more accurate than needed for a given application. But now you are criticizing a practice of weighing evidence too accurately.
>
> I previously agreed that a conjunction has strength at most that of its weakest element. But a probabilistic account gives us more accuracy than that, allowing us to compute a more precise strength (if we have need of it). And similarly, a disjunction has strength at least that of its weakest element -- but a probabilistic account gives us more accuracy than that, if we need it. Quoting you:
>
>> Excess capacity means the idea is more than adequate to accomplish its purpose. The idea is more powerful than necessary to do its job. This lets it deal with variance, and may help with using the idea for other jobs.
>
> Perhaps the excess accuracy in probability theory makes it more powerful than necessary to do its job? Perhaps this helps it deal with variance? Perhaps it helps the idea apply for other jobs than the one it was meant for?
